{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/it-is-nidah/dataset/blob/main/Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 3"
      ],
      "metadata": {
        "id": "T4-dQS60g1aE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1 Develop a CNN from scratch for detection of GAN generated images. Training dataset (150 live\n",
        "and 150 fake images) should be used for CNN training. Trained CNN should be evaluated on test\n",
        "dataset (150 live and 150 fake images). Report Accuracy, F1-score and Recall of the classifier.\n",
        "Dataset:\n",
        "https://www.kaggle.com/datasets/xhlulu/140k-real-and-fake-faces"
      ],
      "metadata": {
        "id": "z3CWxJEVgqIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "r-vmH782bs4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RJo9xJZPeMK",
        "outputId": "8d93942d-79e9-414f-880c-29ba7999cc70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip gdrive/My\\ Drive/dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RS8Vn8DCvoYu",
        "outputId": "e19077d6-ce01-48fe-e3a6-a2ca58e6193f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  gdrive/My Drive/dataset.zip\n",
            "replace real_vs_fake/real-vs-fake/test/fake/00276TOPP4.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data():\n",
        "  real_images_path = \"/content/real_vs_fake/real-vs-fake/train/real\"\n",
        "  fake_images_path = \"/content/real_vs_fake/real-vs-fake/train/fake\"\n",
        "  \n",
        "  real_images = []\n",
        "  fake_images = []\n",
        "\n",
        "  for i in range(1, 151):\n",
        "      real_img = cv2.imread(real_images_path + str(i) + \".jpg\")\n",
        "      fake_img = cv2.imread(fake_images_path + str(i) + \".jpg\")\n",
        "\n",
        "      real_img = cv2.resize(real_img, (128, 128))\n",
        "      fake_img = cv2.resize(fake_img, (128, 128))\n",
        "\n",
        "      real_images.append(real_img)\n",
        "      fake_images.append(fake_img)\n",
        "\n",
        "  real_labels = np.ones(len(real_images))\n",
        "  fake_labels = np.zeros(len(fake_images))\n",
        "\n",
        "  X = np.concatenate((real_images, fake_images), axis=0)\n",
        "  y = np.concatenate((real_labels, fake_labels), axis=0)\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "\n",
        "  return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "LPqQ1WwCcYZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "xLeB-xfJdh_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, X_train, y_train):\n",
        "    history = model.fit(X_train)"
      ],
      "metadata": {
        "id": "ebIz60RzdoHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Use the same CNN architecture trained on any image dataset of your choice. Fine-tune the CNN\n",
        "model on the 150 live and 150 fake images (from the above data source) for the detection of GAN\n",
        "generated images. Fine-tuned CNN should be tested on 150 live and 150 fake images. Report the\n",
        "difference in accuracy, F1-score and recall of the classifier from those observed in Question #1."
      ],
      "metadata": {
        "id": "hTcsiRlogg6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data():\n",
        "    real_images_path = \"/content/real_vs_fake/real-vs-fake/test/real/\"\n",
        "    fake_images_path = \"/content/real_vs_fake/real-vs-fake/test/fake\"\n",
        "\n",
        "    real_images = []\n",
        "    fake_images = []\n",
        "\n",
        "\n",
        "    for i in range(1, 151):\n",
        "        real_img = cv2.imread(real_images_path + str(i) + \".jpg\")\n",
        "        fake_img = cv2.imread(fake_images_path + str(i) + \".jpg\")\n",
        "\n",
        "        real_img = cv2.resize(real_img, (224, 224))\n",
        "        fake_img = cv2.resize(fake_img, (224, 224))\n",
        "\n",
        "        real_images.append(real_img)\n",
        "        fake_images.append(fake_img)\n",
        "\n",
        "    real_labels = np.ones(len(real_images))\n",
        "    fake_labels = np.zeros(len(fake_images))\n",
        "\n",
        "    X = np.concatenate((real_images, fake_images), axis=0)\n",
        "    y = np.concatenate((real_labels, fake_labels), axis=0)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n"
      ],
      "metadata": {
        "id": "Dqvj-MJ_hXs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model():\n",
        "  vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "  x = vgg_model.output\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(256, activation='relu')(x)\n",
        "  x = Dense(128, activation='relu')(x)\n",
        "  predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "  model = Model(inputs=vgg_model.input, outputs=predictions)\n",
        "\n",
        "  for layer in vgg_model.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "wgNeSpSIhd0Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}